# Adaptive UI Agent - Guia Completo de Uso

> **Agente de RL Visual com IntegraÃ§Ã£o LLM**  
> Aprende a executar tarefas visuais a partir de objetivos em linguagem natural.

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
3. [ConfiguraÃ§Ã£o de LLM](#configuraÃ§Ã£o-de-llm)
4. [Uso BÃ¡sico](#uso-bÃ¡sico)
5. [Arquitetura](#arquitetura)
6. [Comandos do Chat](#comandos-do-chat)
7. [Treinamento](#treinamento)
8. [Exemplos PrÃ¡ticos](#exemplos-prÃ¡ticos)
9. [FAQ](#faq)

---

## ğŸ§  VisÃ£o Geral

O Adaptive UI Agent Ã© um sistema que combina:

| Componente | FunÃ§Ã£o |
|------------|--------|
| **LLM** | Interpreta goals â†’ gera planos estruturados |
| **VQ-VAE** | Comprime pixels â†’ representaÃ§Ã£o discreta |
| **PPO** | Aprende polÃ­tica de aÃ§Ãµes via RL |
| **Visual Detectors** | Detecta elementos por pixels |

### PrincÃ­pio Fundamental

```
LLM = cÃ©rebro simbÃ³lico (intenÃ§Ã£o)
RL = cÃ©rebro motor (aÃ§Ã£o)
VQ-VAE = ponte visual (percepÃ§Ã£o)
```

> âš ï¸ **LLM nunca toca no mouse. RL nunca lÃª texto.**

---

## ğŸš€ InstalaÃ§Ã£o

### Requisitos
- Python 3.10+
- CUDA (opcional, para GPU)

### Passos

```bash
# 1. Clone ou acesse o diretÃ³rio
cd adaptive-ui-agent

# 2. Instale dependÃªncias
pip install -r requirements.txt

# 3. Instale LiteLLM para suporte a mÃºltiplos LLMs
pip install litellm

# 4. (Opcional) Instale dependÃªncias de desenvolvimento
pip install -e ".[dev]"
```

### VerificaÃ§Ã£o

```bash
# Testar ambiente
python -c "from env import SandboxEnv; print('Env OK')"

# Testar VQ-VAE
python -c "from vision import VQVAE; print('VQ-VAE OK')"

# Testar Planner
python -c "from planner import create_planner; print('Planner OK')"
```

---

## ğŸ”‘ ConfiguraÃ§Ã£o de LLM

### Passo 1: Copie o arquivo de exemplo

```bash
cp configs/llm_config.example.yaml configs/llm_config.yaml
```

### Passo 2: Configure sua API Key

Edite `configs/llm_config.yaml`:

```yaml
# Escolha seu provider
default_provider: "openai"  # openai, anthropic, google, xai, ollama

# Configure a API key
openai:
  api_key: "sk-sua-chave-aqui"  # Ou use ${OPENAI_API_KEY}
  model: "gpt-5.2-instant"      # Modelo a usar
```

### Providers Suportados

| Provider | Modelos | Notas |
|----------|---------|-------|
| **OpenAI** | gpt-5.2-instant/thinking/pro | Mais evoluÃ­do |
| **Anthropic** | claude-4.5-opus/sonnet/haiku | Contexto longo |
| **Google** | gemini-2.5-flash/pro, gemini-3 | Multimodal |
| **xAI** | grok-4, grok-4.1 | Tempo real |
| **Ollama** | llama4, qwen3, gemma3 | Local/grÃ¡tis |
| **LiteLLM** | 100+ modelos | Universal |

### Via VariÃ¡veis de Ambiente

```bash
# Alternativa: configure via env vars
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GOOGLE_API_KEY="..."
```

---

## ğŸ’» Uso BÃ¡sico

### 1. Modo Interativo (Recomendado)

```bash
python planner/integration.py --interactive
```

```
ğŸ¯ Goal: Cria 3 quadrados azuis alinhados

âœ… Plan generated:
   Goal: create_three_squares
   Elements: 1
   Constraints: 1

ğŸ¯ Starting training...
Episode 50: success_rate=23%, progress=45%
Episode 100: success_rate=67%, progress=82%
Episode 150: success_rate=91%, progress=98%

ğŸ‰ Goal achieved! Success rate: 91%
```

### 2. Goal Ãšnico

```bash
python planner/integration.py --goal "Cria um botÃ£o centralizado"
```

### 3. Com Provider EspecÃ­fico

```bash
# Usar Claude
python planner/integration.py --provider anthropic --interactive

# Usar modelo local
python planner/integration.py --provider ollama --interactive
```

### 4. Script Python

```python
from planner import LLMRLIntegration

# Inicializar
integration = LLMRLIntegration(
    config_path="configs/default.yaml",
    llm_provider="openai"
)

# Treinar em um objetivo
result = integration.train_on_goal(
    "Cria 3 quadrados azuis alinhados",
    max_episodes=500,
    success_threshold=0.8
)

print(f"Sucesso: {result.success}")
print(f"Taxa de sucesso: {result.final_success_rate:.1%}")
```

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADAPTIVE UI AGENT                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚    User     â”‚â”€â”€â”€â–¶â”‚ LLM Planner â”‚â”€â”€â”€â–¶â”‚Structured   â”‚     â”‚
â”‚  â”‚   (texto)   â”‚    â”‚  (GPT/etc)  â”‚    â”‚   Plan      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                               â”‚             â”‚
â”‚                                               â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Actions   â”‚â—€â”€â”€â”€â”‚  PPO Agent  â”‚â—€â”€â”€â”€â”‚ Objective   â”‚     â”‚
â”‚  â”‚  (mouse)    â”‚    â”‚  (policy)   â”‚    â”‚ Translator  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                                â”‚
â”‚         â–¼                  â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Environment â”‚â”€â”€â”€â–¶â”‚   VQ-VAE    â”‚â”€â”€â”€â–¶â”‚Multi-One-Hotâ”‚     â”‚
â”‚  â”‚  (pygame)   â”‚    â”‚  (encoder)  â”‚    â”‚  (18,432-d) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **User** â†’ texto em linguagem natural
2. **LLM** â†’ decompÃµe em plan JSON (usando DSL)
3. **Translator** â†’ converte plan em reward function
4. **Environment** â†’ pixels 64Ã—64 RGB
5. **VQ-VAE** â†’ 6Ã—6 latent â†’ 18,432-dim multi-one-hot
6. **PPO** â†’ aprende polÃ­tica de aÃ§Ãµes
7. **Actions** â†’ 9 aÃ§Ãµes (8 direÃ§Ãµes + click)

---

## ğŸ® Comandos do Chat

Durante o modo interativo:

| Comando | DescriÃ§Ã£o |
|---------|-----------|
| `status` | Mostra estado atual do agente |
| `set_rule <rule>` | Muda regra (blue_bad, blue_good) |
| `swap_targets` | Inverte target/obstacle (Continual RL) |
| `screenshot` | Captura tela atual |
| `reconstruct` | Mostra reconstruÃ§Ã£o VQ-VAE |
| `show_latent` | Visualiza multi-one-hot |
| `step <action>` | Executa aÃ§Ã£o (0-8) |
| `reset` | Reseta ambiente |
| `pause` / `resume` | Controle de treino |
| `help` | Lista comandos |
| `quit` | Sair |

---

## ğŸ¯ Treinamento

### Pipeline Completo

```bash
# Executa: dataset â†’ VQ-VAE â†’ PPO
python scripts/run_training.py
```

### Etapas Individuais

```bash
# 1. Gerar dataset (5000 screenshots)
python scripts/run_training.py --skip-vqvae --skip-ppo

# 2. Treinar VQ-VAE
python scripts/run_training.py --skip-ppo

# 3. Treinar PPO
python scripts/run_training.py --skip-dataset --skip-vqvae
```

### Monitoramento (TensorBoard)

```bash
tensorboard --logdir runs/
```

---

## ğŸ“ Exemplos PrÃ¡ticos

### Exemplo 1: Layout Simples

```python
from planner import LLMRLIntegration

agent = LLMRLIntegration()
result = agent.train_on_goal("3 retÃ¢ngulos azuis em linha")
# Agente aprende a criar e alinhar elementos
```

### Exemplo 2: Continual RL

```python
# Treina primeira regra
agent.train_on_goal("Clica no quadrado azul")

# Muda regra (continual RL)
agent.env.swap_targets()

# Agente re-adapta rapidamente
agent.train_on_goal("Clica no quadrado vermelho", max_episodes=200)
```

### Exemplo 3: Com LLM Real

```python
from planner import LLMRLIntegration
from planner.llm_provider import create_llm_provider

# Usar GPT-5.2
provider = create_llm_provider(provider="openai", model="gpt-5.2-thinking")

agent = LLMRLIntegration(llm_provider=provider)
result = agent.train_on_goal("Cria uma interface de login simples")
```

---

## â“ FAQ

### Q: O agente "entende" o que estÃ¡ fazendo?
**A:** NÃ£o. Ele aprende associaÃ§Ãµes visuais â†’ aÃ§Ãµes â†’ recompensas. NÃ£o hÃ¡ compreensÃ£o semÃ¢ntica.

### Q: Posso usar sem GPU?
**A:** Sim. CPU funciona, mas treinamento Ã© mais lento.

### Q: Como adicionar novo provider LLM?
**A:** Edite `configs/llm_config.yaml` e use o formato LiteLLM: `provider/model`.

### Q: O agente pode criar interfaces reais?
**A:** Em teoria, sim (com Figma em VM). Na prÃ¡tica, comeÃ§a com sandbox simples.

### Q: Qual a diferenÃ§a para RPA tradicional?
**A:** RPA usa scripts fixos. Este agente **aprende** polÃ­ticas a partir de pixels.

---

## ğŸ“ Estrutura do Projeto

```
adaptive-ui-agent/
â”œâ”€â”€ agent/              # PPO implementation
â”‚   â”œâ”€â”€ ppo.py          # Policy/Value networks
â”‚   â””â”€â”€ trainer.py      # Training orchestrator
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml    # Hyperparameters
â”‚   â””â”€â”€ llm_config.yaml # LLM API keys
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ sandbox_env.py  # Pygame environment
â”‚   â””â”€â”€ extended_sandbox.py
â”œâ”€â”€ interaction/
â”‚   â”œâ”€â”€ chat_interface.py
â”‚   â””â”€â”€ visualizer.py
â”œâ”€â”€ planner/            # LLM-RL integration
â”‚   â”œâ”€â”€ goal_dsl.py     # Visual vocabulary
â”‚   â”œâ”€â”€ visual_detectors.py
â”‚   â”œâ”€â”€ reward_generator.py
â”‚   â”œâ”€â”€ llm_planner.py
â”‚   â”œâ”€â”€ llm_provider.py # Universal LLM (LiteLLM)
â”‚   â””â”€â”€ integration.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_training.py
â”‚   â””â”€â”€ demo.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_env.py
â”‚   â”œâ”€â”€ test_vqvae.py
â”‚   â””â”€â”€ test_ppo.py
â””â”€â”€ vision/
    â”œâ”€â”€ vqvae.py
    â””â”€â”€ train_vqvae.py
```

---

## ğŸ”— Links Ãšteis

- **Paper base**: arXiv 2312.01203v3
- **LiteLLM docs**: https://docs.litellm.ai
- **TensorBoard**: localhost:6006 (apÃ³s `tensorboard --logdir runs/`)

---

*Ãšltima atualizaÃ§Ã£o: Janeiro 2026*
